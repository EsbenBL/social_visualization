{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "As explained in the [Before week 1: How to take this class](https://nbviewer.org/github/suneman/socialdata2023/blob/main/lectures/How_To_Take_This_Class.ipynb) notebook, each week of this class is an Jupyter notebook like this one. In order to follow the class, you simply start reading from the top, following the instructions.\n",
    "\n",
    "**New Info**: Remember that this week is also the time to learn a bit about how the the assignments and the final project work. So if you havn't already, check out the [Before week 2: Assignments and Final Project](https://github.com/suneman/socialdata2023/blob/main/lectures/Assignments_And_Final_Project.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today's lecture does a few things.\n",
    "* First there is an introduction to data visualization incl a little exercise and a video (Part 1). \n",
    "* As the main event, we will work with crime-data and generate a large number of interesting and informative plots (Part 2,4,5).\n",
    "* We will also talk a bit about what makes a good plot (Part 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A little visualization exercise\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data1.tsv), [Data 2](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data2.tsv), [Data 3](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data3.tsv), and [Data 4](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "As you will later realize, these are famous datasets!\n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download these files to disk by right-clicking on each one, but if you use Python and `urllib` or `urllib2` to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend.\n",
    "\n",
    "Now, to the exercise:\n",
    "\n",
    "> *Exercise*: \n",
    "> \n",
    "> * Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    ">      * Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. \n",
    "> * Now calculate the variance for all of the various sets of $x$- and $y$-values, by using the `numpy` function `var`. Print it to three decimal places.\n",
    "> * Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also print to three decimal places).\n",
    "> * The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "> ```\n",
    "> from scipy import stats\n",
    "> slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    ">```\n",
    "> * Comment on the results from the previous steps. What do you observe? \n",
    "> * Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/anscombe.png).)\n",
    "> * Explain - in your own words - what you think my point with this exercise is (see below for tips on this).\n",
    "\n",
    "\n",
    "Get more insight in the ideas behind this exercise by reading [here](https://en.wikipedia.org/wiki/Anscombe%27s_quartet). Here you can also get an explanation of why the datasets are actually famous - I mean they have their own Wikipedia page!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you get a better sense of why data visualization is an important and powerful tool, you are ready to get a small intro on the topic! Again, don't watch the video until **after** you've done exercise 1.1 \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/9D2aI30AMhM/0.jpg)](https://www.youtube.com/watch?v=9D2aI30AMhM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise:* Questions for the lecture\n",
    "> * What is the difference between *data* and *metadata*? How does that relate to the GPS tracks-example?\n",
    "> * Sune says that the human eye is a great tool for data analysis. Do you agree? Explain why/why not. Mention something that the human eye is very good at. Can you think of something that [is difficult for the human eye](http://cdn.ebaumsworld.com/mediaFiles/picture/718392/84732652.jpg). Explain why your example is difficult. \n",
    "> * Simpson's paradox is hard to explain. Come up with your own example - or find one on line.\n",
    "> * In your own words, explain the differnece between *exploratory* and *explanatory* data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Exercise 1 #######\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams[\"font.family\"] = \"Georgia\"\n",
    "\n",
    "## Load data\n",
    "root_url = \"https://raw.githubusercontent.com/suneman/socialdata2023/main/files/\"\n",
    "data1 = pd.read_csv(f\"{root_url}data1.tsv\", sep = \"\\t\", header = None, names = [\"x\",\"y\"])\n",
    "data2 = pd.read_csv(f\"{root_url}data2.tsv\", sep = \"\\t\", header = None, names = [\"x\",\"y\"])\n",
    "data3 = pd.read_csv(f\"{root_url}data3.tsv\", sep = \"\\t\", header = None, names = [\"x\",\"y\"])\n",
    "data4 = pd.read_csv(f\"{root_url}data4.tsv\", sep = \"\\t\", header = None, names = [\"x\",\"y\"])\n",
    "\n",
    "## Calc means\n",
    "print(\"Calculating mean-values:\\n\")\n",
    "for idx, data in enumerate([data1, data2, data3, data4]):\n",
    "    x_mean = np.mean(data.x)\n",
    "    y_mean = np.mean(data.y)\n",
    "    print(f\"mean x in dataset {idx+1}: {x_mean:.2f}, and mean y in dataset {idx+1}: {y_mean:.2f}\")\n",
    "\n",
    "## Calc variance\n",
    "print(\"\\nCalculating variance:\\n\")\n",
    "for idx, data in enumerate([data1, data2, data3, data4]):\n",
    "    x_var = np.var(data.x, ddof = 1)\n",
    "    y_var = np.var(data.y, ddof = 1)\n",
    "    print(f\"variance x dataset {idx+1}: {x_var:.3f}, and variance y in dataset {idx+1}: {y_var:.3f}\")\n",
    "\n",
    "## Calc correlation\n",
    "print(\"\\nCalculating correlation:\\n\")\n",
    "for idx, data in enumerate([data1, data2, data3, data4]):\n",
    "    pear_corr = np.corrcoef(data.x, data.y)[0][1]\n",
    "    print(f\"Pearson Correlation between x and y in dataset {idx+1}: {pear_corr:.3f}\")\n",
    "\n",
    "## Linear regression\n",
    "print(\"\\nLinear regression:\\n\")\n",
    "for idx, data in enumerate([data1, data2, data3, data4]):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data.x,data.y)\n",
    "    print(f\"Intercept (b) / slope (a) of linear regression on dataset {idx+1}: {intercept:.3f} / {slope:.3f}\")\n",
    "\n",
    "## Comment on Regression\n",
    "'''We see a relatively small positive slope coefficient, indicating that a unit increase in x results in an increase of 0.5 units in y. This \n",
    "is the case in all 4 datasets. The intercept is approx 3 in all 4 dataset. All in all, all aggregate statistics are the same for the 4 datasets'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot scatter and regline\n",
    "def plot_scatter_with_reg(data, _ax, title):\n",
    "    _ax.scatter(data.x, data.y, color = \"black\")\n",
    "    m, b = np.polyfit(data.x, data.y, 1)\n",
    "    _ax.plot(data.x, m*data.x+b, color = \"red\")\n",
    "    _ax.set_ylabel(\"y\")\n",
    "    _ax.set_xlabel(\"x\")\n",
    "    _ax.set_title(title)\n",
    "    \n",
    "\n",
    "# create subplots\n",
    "fig, ax = plt.subplots(2,2, figsize = (10,6))\n",
    "# set parameters - same xlim and ylim\n",
    "plt.setp(ax, xlim=[0,20], ylim=[2,14])\n",
    "\n",
    "plot_scatter_with_reg(data1, ax[0][0], title = \"Dataset 1\")\n",
    "plot_scatter_with_reg(data2, ax[0][1], title = \"Dataset 2\")\n",
    "plot_scatter_with_reg(data3, ax[1][0], title = \"Dataset 3\")\n",
    "plot_scatter_with_reg(data4, ax[1][1], title = \"Dataset 4\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the excercie**:\n",
    "\n",
    "You can't necesarrily trust aggregate statistics - You can tell that the distribution of the individual datapoints are quite different, but aggregated, all dataset look alike. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing patterns in the data\n",
    "\n",
    "Visualizing data is a powerful technique that helps us exploiting the human eye, and make complex patterns easier to identify. \n",
    "\n",
    "Let's see if we can detect any interesting patterns in the big crime-data file from San Francisco you downloaded last week. We'll again only look at the focus-crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: More temporal patterns. Last time we plotted the development over time (how each of the focus crimes changed over time, year-by-year). Today we'll start by looking at the developments across the months, weekdays, and across the 24 hours of the day. \n",
    ">\n",
    "> **Note:** restrict yourself to the dataset of *entire years*.\n",
    ">\n",
    "> * *Weekly patterns*. Basically, we'll forget about the yearly variation and just count up what happens during each weekday. [Here's what my version looks like](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/weekdays.png). Some things make sense - for example `drunkenness` and the weekend. But there are some aspects that were surprising to me. Check out `prostitution` and mid-week behavior, for example!?\n",
    "> * *The months*. We can also check if some months are worse by counting up number of crimes in Jan, Feb, ..., Dec. Did you see any surprises there?\n",
    "> * *The 24 hour cycle*. We can also forget about weekday and simply count up the number of each crime-type that occurs in the dataset from midnight to 1am, 1am - 2am ... and so on. Again: Give me a couple of comments on what you see. \n",
    "> * *Hours of the week*. But by looking at just 24 hours, we may be missing some important trends that can be modulated by week-day, so let's also check out the 168 hours of the week. So let's see the number of each crime-type Monday night from midninght to 1am, Monday night from 1am-2am - all the way to Sunday night from 11pm to midnight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Load, specify columns and convert to datatime\n",
    "report_data = pd.read_csv(\"../Datasets/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")\n",
    "report_data = report_data[['Category', 'Date', 'Time']]\n",
    "report_data[\"Date\"] = pd.to_datetime(report_data.Date, format = \"%m/%d/%Y\")\n",
    "report_data[\"Hour\"] = pd.to_datetime(report_data[\"Time\"], format=\"%H:%M\").dt.hour\n",
    "\n",
    "# Extract relevant time-periods\n",
    "report_data[\"Weekday\"] = report_data[\"Date\"].dt.day_name()\n",
    "report_data[\"Month\"] = report_data[\"Date\"].dt.month_name()\n",
    "report_data[\"12_hours\"] = [datetime.strftime(datetime.strptime(str(t), \"%H\"), \"%I %p\") for t in report_data[\"Hour\"] ]\n",
    "report_data[\"Weekday_Hour\"] = report_data[\"Weekday\"] + \"-\" + report_data[\"12_hours\"] \n",
    "\n",
    "# Order \n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "hour_order = [datetime.strftime(datetime.strptime(str(t), \"%H\"), \"%I %p\") for t in range(24)]\n",
    "weekday_hour_order = [weekday + \"-\" + hour for weekday in weekday_order for hour in hour_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to blot n reported crimes in a focus-category into a timeperiod\n",
    "def subset_and_aggregate_data(data, crime_type, period_col, xtick_order):\n",
    "    sub_data = data.query(f\"Category == '{crime_type}'\")\n",
    "    reports_in_periods = sub_data.value_counts(period_col)\n",
    "    return reports_in_periods.reindex(xtick_order)\n",
    "\n",
    "def make_plot(data, crime_type, period_col, xtick_order, ax, exclude_xticks = True):\n",
    "    reports_in_periods = subset_and_aggregate_data(data, crime_type, period_col, xtick_order)\n",
    "    reports_in_periods.plot.bar(title = crime_type.title(), ax=ax, color = \"skyblue\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Crimes reported\")\n",
    "    if exclude_xticks:\n",
    "        ax.set_xticks([])\n",
    "    \n",
    "\n",
    "def update_axes_idx(itr, ax_row_idx, ax_col_idx):\n",
    "    if (itr % 2) == 1: # after uneven iterations - new row, reset col_idx\n",
    "        ax_row_idx += 1\n",
    "        ax_col_idx = 0\n",
    "    else: # after even iterations - Next column\n",
    "        ax_col_idx += 1\n",
    "\n",
    "    return ax_row_idx, ax_col_idx\n",
    "\n",
    "\n",
    "def plot_reports_in_periods(data, period_col, xtick_order, x_label, title, n_fig_rows, n_fig_cols):\n",
    "    fig, ax = plt.subplots(n_fig_rows, n_fig_cols, figsize = (8, 20))\n",
    "    exclude_xticks = True\n",
    "\n",
    "    ax_row_idx, ax_col_idx = 0, 0 # indexes for the \"ax\"-attribute\n",
    "\n",
    "    for itr, cat in enumerate(focuscrimes):\n",
    "        make_plot(report_data, crime_type = cat, period_col = period_col, xtick_order = xtick_order, ax = ax[ax_row_idx][ax_col_idx], exclude_xticks = exclude_xticks)\n",
    "        ax_row_idx, ax_col_idx = update_axes_idx(itr, ax_row_idx, ax_col_idx)\n",
    "        if (ax_row_idx+1) == n_fig_rows:\n",
    "            exclude_xticks = False\n",
    "\n",
    "    ax[6][0].set_xlabel(x_label)\n",
    "    ax[6][1].set_xlabel(x_label)\n",
    "\n",
    "    plt.suptitle(title, y = 1.005, size = 20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reports_in_periods(report_data, period_col = \"Weekday\", xtick_order = weekday_order, x_label = \"Weekday\", title = \"N. crimes pr. weekday by category\", n_fig_rows = 7, n_fig_cols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reports_in_periods(report_data, period_col = \"Month\", xtick_order = month_order, x_label = \"Month\", title = \"N. crimes pr. month by category\", n_fig_rows = 7, n_fig_cols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reports_in_periods(report_data, period_col = \"12_hours\", xtick_order = hour_order, x_label = \"Hour\", title = \"N. crimes pr. hour by category\", n_fig_rows = 7, n_fig_cols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reports_in_periods(report_data, period_col = \"Weekday_Hour\", xtick_order = weekday_hour_order, x_label = \"Weekday and Hour\", title = \"N. crimes pr. hour in a weekday by category\", n_fig_rows = 7, n_fig_cols = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating nice plots\n",
    "\n",
    "Ok. There's a lot of barcharts today. We need them ... they are a fantastic tool for data exploration. But it can get monotonous, so let's take a little break to talk about something else before digging deeper with the barcharts.\n",
    "\n",
    "I want to tell you a bit about how to make nice plots. I do that in the video below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise:* Nice plots\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Exploring other types of plots for temporal data\n",
    "\n",
    "We continue our mini-break from barcharts by exploring more ways to plot temporal data.\n",
    "\n",
    "> *Exercise (extra hard):* Other cool ways to plot temoral data. This exercise is a bit more free than the previous ones. I am going to introduce three different plot-types. Then your job is to choose a part of the crime-data that you care about - and plot it using these new ways of visualizing data. \n",
    ">\n",
    ">I recommend that you choose a different part of the crime-data for each plot-type.\n",
    "> * Calendar plots. Get started on calendar plots **[here](https://calplot.readthedocs.io/en/latest/)**. There are other packages for plotting these, those are also OK to use.\n",
    "> * [Polar bar chart](https://user-images.githubusercontent.com/12328192/89272649-be76e200-d63e-11ea-97ad-fd1ba5831c89.png). Here I want you to plot a 24-hour pattern of some sort -- those work really well in radial plots (another name for polar plots) because the day  wraps around on itself. You can also try plotting data with patterns from the 168 hours of the week. There's not one super-awesome solution here, you can try using [pure matplotlib](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_bar.html) ... [some examples here](https://www.python-graph-gallery.com/circular-barplot/) or via [plotly](https://plotly.com/python/polar-chart/) (scroll down a bit for the polar barchart).\n",
    "> * Time series. Time series is a key functionality of `Pandas`, so here I simply recommend starting by searching your favorite search engine for something like `time series` `pandas`\n",
    "\n",
    "**Note**: I added this exercise with fewer hints than usual to give you a bit of an extra challenge. Once you're done, you'll agree that it is not difficult to create these plots. What ***IS*** difficult is figuring out all the little steps you need to do to make them work. \n",
    "\n",
    "*My philosophy for data science is this*: Getting to what you want rarely seems hard once you found your way there, the difficulty comes in breaking down a hard problem into the little steps you need to take to solve your complex problem. In this class, I usually do the breaking down for you and provide you with the steps (that's how you go from nothing to creating complex visualizations of crime-data). But I also want you to learn the breaking-problems-down part. So this time I'm trying to do that by posing a slightly more high-level set of problems than usual. \n",
    "\n",
    "*My approach is always to think*: Even if my task seems impossible, I think is there any problem that I ***CAN*** solve that will get me closer to where I want to go. Once I've solved that part, I'm smarter and I try to think: Is there a new problem I can solve that'll get me closer knowing what I know now. And I just keep going. Usually that's enough.\n",
    "\n",
    "*If that seems too abstract*, a useful goal for you is to use your internet searching skills to figure out how to make each visualization work -- look for examples, tutorials, stack overflow posts, people who have found the same error messages as you, etc, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calplot\n",
    "plt.rcParams[\"font.family\"] = \"Georgia\"\n",
    "\n",
    "calplot_data = report_data.assign(Year = report_data.Date.dt.year).query(\"Year <= 2005\").\\\n",
    "    query(\"Category == 'DRUNKENNESS'\").groupby(\"Date\").count().Category\n",
    "\n",
    "calplot.calplot(calplot_data, cmap  = \"ocean\", colorbar = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calplot.yearplot(calplot_data, year=2003);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "test_data = report_data.value_counts(\"12_hours\").reindex(hour_order).reset_index().rename(columns = {0:\"count\"})\n",
    "fig = px.scatter_polar(test_data, r=\"count\", theta=\"12_hours\")\n",
    "fig.update_traces(fill='toself')\n",
    "fig.update_layout(\n",
    "    title = \"CRIME! around the clock\",\n",
    "    font_size = 15,\n",
    "    paper_bgcolor = \"rgb(223, 223, 223)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Back to visualizing patterns in the data\n",
    "\n",
    "The next thing we'll be looking into is how crimes break down across the 10 districts in San Francisco.\n",
    "\n",
    "> *Exercise*: The types of crime and how they take place across San Francisco's police districts.\n",
    ">  \n",
    ">  * So now we'll be combining information about `PdDistrict` and `Category` to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts.\n",
    ">  * Which has the most crimes? Which has the most focus crimes?\n",
    ">  * Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going:\n",
    ">    - First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/CrimeOccurrencesByCategory.png). Let's call it `P(crime)`.\n",
    ">    - Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`.\n",
    ">    - Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole.\n",
    ">    - For each district plot these ratios for the 14 focus crimes. My plot looks like this\n",
    "> ![Histograms](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/conditional.png \"histograms\")\n",
    ">    - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia?\n",
    "\n",
    "**Comment**. Notice how much awesome data science (i.e. learning about interesting real-world crime patterns) we can get out by simply counting and plotting (and looking at ratios). Pretty great, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Excercise 5 #### #\n",
    "report_data = pd.read_csv(\"../Datasets/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")\n",
    "report_data = report_data[-report_data[\"PdDistrict\"].isna()]\n",
    "\n",
    "unique_district = report_data[\"PdDistrict\"].unique()\n",
    "print(\"The 10 districts:\\n\",report_data[\"PdDistrict\"].unique())\n",
    "\n",
    "print(f\"Most overall crime district: {report_data['PdDistrict'].value_counts().idxmax()}\")\n",
    "print(f\"Most focus-crime district: {report_data[report_data.Category.isin(focuscrimes)]['PdDistrict'].value_counts().idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_report_data = report_data[report_data.Category.isin(focuscrimes)]\n",
    "p_crime = focus_report_data.Category.value_counts(normalize=True).reset_index()\n",
    "p_crime.columns = [\"Category\", \"Prob\"]\n",
    "\n",
    "n_fig_rows = 5\n",
    "fig, ax = plt.subplots(n_fig_rows,2, figsize = (12, 18))\n",
    "\n",
    "ax_row_idx, ax_col_idx = 0, 0 # indexes for the \"ax\"-attribute\n",
    "\n",
    "for itr, district in enumerate(unique_district):\n",
    "    # subset district data\n",
    "    district_data = focus_report_data.query(f\"PdDistrict == '{district}'\").copy() \n",
    "    # calculate probability in district\n",
    "    p_crime_district = district_data.Category.value_counts(normalize=True).reset_index()\n",
    "    p_crime_district.columns = [\"Category\", \"Prob_district\"]\n",
    "    # merge overall probability onto district and calculate ratio\n",
    "    p_crime_district = p_crime_district.merge(p_crime, on = \"Category\").sort_values(\"Category\")\n",
    "    p_crime_district[\"p_crime_dist_ration\"] = p_crime_district[\"Prob_district\"] / p_crime_district[\"Prob\"]\n",
    "    \n",
    "    # Plot \n",
    "    if (ax_row_idx+1) == n_fig_rows:\n",
    "        p_crime_district[[\"p_crime_dist_ration\", \"Category\"]].set_index(\"Category\", drop=True).plot.bar(title = district.title(),legend = False, color = \"skyblue\", ax = ax[ax_row_idx][ax_col_idx])\n",
    "        ax[ax_row_idx][ax_col_idx].set_xlabel(\"Category\")\n",
    "        \n",
    "    else:\n",
    "        p_crime_district[[\"p_crime_dist_ration\", \"Category\"]].set_index(\"Category\", drop=True).plot.bar(title = district.title(),legend = False, color = \"skyblue\", ax = ax[ax_row_idx][ax_col_idx])\n",
    "        ax[ax_row_idx][ax_col_idx].set_xticks([])\n",
    "        ax[ax_row_idx][ax_col_idx].set_xlabel(\"\")\n",
    "\n",
    "    ax[ax_row_idx][ax_col_idx].axhline(y = 1, color = 'gray', linestyle = '--')\n",
    "    ax[ax_row_idx][ax_col_idx].set_ylabel(\"ratio\")\n",
    "    \n",
    "    ax_row_idx, ax_col_idx = update_axes_idx(itr, ax_row_idx, ax_col_idx)\n",
    "\n",
    "plt.suptitle(\"REMEMBER A TITLE\", y = 1.005, size = 20)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esbens-github-TfeufWp2-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8392c8f4c86f9c2142d231d5e3dcdde86b440e8f65793d5a51bba966016be36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
